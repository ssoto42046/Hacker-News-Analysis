{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Analyzing Hacker News Data\n",
    "In this project, we are interested in specific types of posts on HackerNews' site. We want to analyze Ask HN or Show HN post types. From there we want to uncover any trends or perform some surface level level analysis in order to come up with some decision about the information\n",
    "\n",
    "***\n",
    "\n",
    "## The Data Set\n",
    "Our data set is from [here](https://www.kaggle.com/hacker-news/hacker-news-posts), the original data has about 300,000 rows, but it was reduced to about 20,000 in order to reduce our computational complexity and save time.\n",
    "\n",
    "The rows of data follow this structure:\n",
    "\n",
    "| Column | Meaning |\n",
    "| :---: | :---: |\n",
    "| id | unique identifier for post |\n",
    "| title | title of post | \n",
    "| url | URL that the post links to, given the post has a URL |\n",
    "| num_points | number of points the post got, total upvotes - total downvotes|\n",
    "| num_comments | number of comments on post |\n",
    "| author | username of person who submit the post |\n",
    "| created_at | date and time at which post was submitted | \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['id', 'title', 'url', 'num_points', 'num_comments', 'author', 'created_at'], ['12224879', 'Interactive Dynamic Video', 'http://www.interactivedynamicvideo.com/', '386', '52', 'ne0phyte', '8/4/2016 11:52'], ['10975351', 'How to Use Open Source and Shut the Fuck Up at the Same Time', 'http://hueniverse.com/2016/01/26/how-to-use-open-source-and-shut-the-fuck-up-at-the-same-time/', '39', '10', 'josep2', '1/26/2016 19:30'], ['11964716', \"Florida DJs May Face Felony for April Fools' Water Joke\", 'http://www.thewire.com/entertainment/2013/04/florida-djs-april-fools-water-joke/63798/', '2', '1', 'vezycash', '6/23/2016 22:20'], ['11919867', 'Technology ventures: From Idea to Enterprise', 'https://www.amazon.com/Technology-Ventures-Enterprise-Thomas-Byers/dp/0073523429', '3', '1', 'hswarna', '6/17/2016 0:01']]\n"
     ]
    }
   ],
   "source": [
    "# reading in and printing out first five rows\n",
    "from csv import reader\n",
    "opened_file = open('hacker_news.csv')\n",
    "read_file = reader(opened_file)\n",
    "hn = list(read_file)\n",
    "print(hn[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "headers = hn[0]\n",
    "del hn[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above cell, we place the header rows into a different list, that is to not have to iterate through it all the time when we are doing our cleaning and analysis. We also make sure that we run this cell only once as we do not want to delete the zeroth item many times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'title', 'url', 'num_points', 'num_comments', 'author', 'created_at']\n",
      "[['12224879', 'Interactive Dynamic Video', 'http://www.interactivedynamicvideo.com/', '386', '52', 'ne0phyte', '8/4/2016 11:52'], ['10975351', 'How to Use Open Source and Shut the Fuck Up at the Same Time', 'http://hueniverse.com/2016/01/26/how-to-use-open-source-and-shut-the-fuck-up-at-the-same-time/', '39', '10', 'josep2', '1/26/2016 19:30'], ['11964716', \"Florida DJs May Face Felony for April Fools' Water Joke\", 'http://www.thewire.com/entertainment/2013/04/florida-djs-april-fools-water-joke/63798/', '2', '1', 'vezycash', '6/23/2016 22:20'], ['11919867', 'Technology ventures: From Idea to Enterprise', 'https://www.amazon.com/Technology-Ventures-Enterprise-Thomas-Byers/dp/0073523429', '3', '1', 'hswarna', '6/17/2016 0:01'], ['10301696', 'Note by Note: The Making of Steinway L1037 (2007)', 'http://www.nytimes.com/2007/11/07/movies/07stein.html?_r=0', '8', '2', 'walterbell', '9/30/2015 4:12']]\n"
     ]
    }
   ],
   "source": [
    "# we look at the header and also the first 5 rows of the actual data\n",
    "print(headers)\n",
    "print(hn[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1744\n",
      "1162\n",
      "17194\n"
     ]
    }
   ],
   "source": [
    "ask_posts = []\n",
    "show_posts = []\n",
    "other_posts = []\n",
    "for row in hn:\n",
    "    title = row[1]\n",
    "    if title.lower().startswith('ask hn'):\n",
    "        ask_posts.append(row)\n",
    "    elif title.lower().startswith('show hn'):\n",
    "        show_posts.append(row)\n",
    "    else:\n",
    "        other_posts.append(row)\n",
    "print(len(ask_posts))\n",
    "print(len(show_posts))\n",
    "print(len(other_posts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments on posts\n",
    "We are also interested in determing which type of posts have more comments on average. We iterate over each of the lists and aggregate the total number of comments in all of the posts and divide by number of posts in each to find the average for the different types of posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average ask comments: 14.038417431192661\n",
      "Average show comments: 10.31669535283993\n"
     ]
    }
   ],
   "source": [
    "total_ask_comments = 0\n",
    "total_show_comments = 0\n",
    "for ask in ask_posts:\n",
    "    n_comments = int(ask[4])\n",
    "    total_ask_comments += n_comments\n",
    "for show in show_posts:\n",
    "    n_comments = int(show[4])\n",
    "    total_show_comments += n_comments\n",
    "avg_ask_comments = total_ask_comments/len(ask_posts)\n",
    "avg_show_comments = total_show_comments/len(show_posts)\n",
    "print('Average ask comments:',avg_ask_comments)\n",
    "print('Average show comments:',avg_show_comments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on our caclulations, we see that ask posts recieve more comments on average than do show posts by about 4 more posts. This is consistent given there are more ask posts in general, perhaps it is safe to say that they have more average interactions per user. Hence, we see this difference in the average number of comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 447, 1: 683, 2: 1381, 3: 421, 4: 337, 5: 464, 6: 397, 7: 267, 8: 492, 9: 251, 10: 793, 11: 641, 12: 687, 13: 1253, 14: 1416, 15: 4477, 16: 1814, 17: 1146, 18: 1439, 19: 1188, 20: 1722, 21: 1745, 22: 479, 23: 543}\n",
      "\n",
      "\n",
      "{0: 55, 1: 60, 2: 58, 3: 54, 4: 47, 5: 46, 6: 44, 7: 34, 8: 48, 9: 45, 10: 59, 11: 58, 12: 73, 13: 85, 14: 107, 15: 116, 16: 108, 17: 100, 18: 109, 19: 110, 20: 80, 21: 109, 22: 71, 23: 68}\n"
     ]
    }
   ],
   "source": [
    "import datetime as dt\n",
    "result_list = []\n",
    "for post in ask_posts:\n",
    "    result_list.append((post[6], int(post[4])))\n",
    "\n",
    "counts_by_hour = {}\n",
    "comments_by_hour = {}\n",
    "\n",
    "for row in result_list:\n",
    "    date = row[0]\n",
    "    time = dt.datetime.strptime(date, '%m/%d/%Y %H:%M')\n",
    "    hours = time.hour\n",
    "    if hours not in counts_by_hour:\n",
    "        counts_by_hour[hours] = 1\n",
    "        comments_by_hour[hours] = row[1]\n",
    "    else:\n",
    "        counts_by_hour[hours]+=1\n",
    "        comments_by_hour[hours]+=row[1]\n",
    "print(comments_by_hour)\n",
    "print('\\n')\n",
    "print(counts_by_hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 8.127272727272727), (1, 11.383333333333333), (2, 23.810344827586206), (3, 7.796296296296297), (4, 7.170212765957447), (5, 10.08695652173913), (6, 9.022727272727273), (7, 7.852941176470588), (8, 10.25), (9, 5.5777777777777775), (10, 13.440677966101696), (11, 11.051724137931034), (12, 9.41095890410959), (13, 14.741176470588234), (14, 13.233644859813085), (15, 38.5948275862069), (16, 16.796296296296298), (17, 11.46), (18, 13.20183486238532), (19, 10.8), (20, 21.525), (21, 16.009174311926607), (22, 6.746478873239437), (23, 7.985294117647059)]\n"
     ]
    }
   ],
   "source": [
    "avg_by_hour = []\n",
    "for time in counts_by_hour:\n",
    "    avg_by_hour.append((time,comments_by_hour[time]/counts_by_hour[time]))\n",
    "print(avg_by_hour)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wish to make the averages for the corresponding hour look a bit nicer, so below I print and format them so that the information is more readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hour: 0 Average: 8.127272727272727\n",
      "Hour: 1 Average: 11.383333333333333\n",
      "Hour: 2 Average: 23.810344827586206\n",
      "Hour: 3 Average: 7.796296296296297\n",
      "Hour: 4 Average: 7.170212765957447\n",
      "Hour: 5 Average: 10.08695652173913\n",
      "Hour: 6 Average: 9.022727272727273\n",
      "Hour: 7 Average: 7.852941176470588\n",
      "Hour: 8 Average: 10.25\n",
      "Hour: 9 Average: 5.5777777777777775\n",
      "Hour: 10 Average: 13.440677966101696\n",
      "Hour: 11 Average: 11.051724137931034\n",
      "Hour: 12 Average: 9.41095890410959\n",
      "Hour: 13 Average: 14.741176470588234\n",
      "Hour: 14 Average: 13.233644859813085\n",
      "Hour: 15 Average: 38.5948275862069\n",
      "Hour: 16 Average: 16.796296296296298\n",
      "Hour: 17 Average: 11.46\n",
      "Hour: 18 Average: 13.20183486238532\n",
      "Hour: 19 Average: 10.8\n",
      "Hour: 20 Average: 21.525\n",
      "Hour: 21 Average: 16.009174311926607\n",
      "Hour: 22 Average: 6.746478873239437\n",
      "Hour: 23 Average: 7.985294117647059\n"
     ]
    }
   ],
   "source": [
    "for thing in avg_by_hour:\n",
    "    print('Hour:', thing[0], 'Average:', thing[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at Best Times to Post\n",
    "We seek to look at the top 5 best times to post based on the average number of comments per hour. So we must sort our information and then look at the 5 elements of the information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Hours for Ask Posts Coments\n",
      "15:0: 38.59 average coments per post\n",
      "2:0: 23.81 average coments per post\n",
      "20:0: 21.52 average coments per post\n",
      "16:0: 16.80 average coments per post\n",
      "21:0: 16.01 average coments per post\n"
     ]
    }
   ],
   "source": [
    "swap_avg_by_hour = []\n",
    "for row in avg_by_hour:\n",
    "    swap_avg_by_hour.append((row[1], row[0]))\n",
    "sorted_swap = sorted(swap_avg_by_hour,reverse=True)\n",
    "print('Top 5 Hours for Ask Posts Coments')\n",
    "for thing in sorted_swap[0:5]:\n",
    "    print('{hr}:{min}: {avg:.2f} average coments per post'.format(hr=dt.datetime.strptime(str(thing[1]),'%H').hour, min=dt.datetime.strptime(str(thing[1]),'%H').minute,avg=thing[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of Information\n",
    "### Time Zone Difference\n",
    "Before our analysis, it is important to know the time zone of the acquired data. The documentation notes that the data set time is in EST (Eastern Time US). This means that this data is 3 hours ahead of my personal time PST (Pacific Standard Time). \n",
    "### Analysis\n",
    "Based on our information, we must take away 3 hours to find the corresponding time that is relevant to me. We see that the best time to post and get the most interactions is at 12PM. This is possible because people might be on break, so use this time wisely!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
